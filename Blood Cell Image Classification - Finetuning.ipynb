{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook runs through an implementation of image classification using Keras. The dataset being used is the Blood Cells dataset. The goal is to create a Convolutional Neural Network that can classify differet forms of blood cells. This notebook demonstrates an example of finetuning a neural network. The premade VGG19 architecture and weights are used, but several of the layers are unfrozen and trained. In addition, several new layers are added to the architecture.\n",
    "\n",
    "The dataset comes with a subset of images that have been augmented and contain additional label subtypes. It uses the \"Dataset2-master\" folder, whihc has 2500 images (augmented from the original images. The four cell types are: Neutrophil, Eosinophil, Monocyte, and Lymphocyte. (Some cell images in the database have more than one nucleus, so they have two labels instead of one, but for the moment we are concerned only with classifying the four primary classes. Just keep in mind accuracy would improve if we accounted for these.)\n",
    "\n",
    "The accuracy demonstrated in this notbeook won't be that great, but this notebook is primarily concerned with demonstrating how to implement a pretrained network and fine-tune it. Try playing with the model and training arguments to see what kind of accuracy you can get from it. Try varying image size by shifting the target_size and input_shape paramaters. The larger the images being put into the model, the more the model will learn, up to the total size of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we will start by importing all the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "import os\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using the built-in Keras ImageDataGenerator to get the image data from our directory. It's a handy function that can collect and sort image data easily, saving a lot of manual preprocessing and data handling. We need to set the directories we are drawing the trainign data from, and then we need to create instances of the ImageDataGenerator. We'll also specify some arguments for later use, like the batch size and number of classes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = \"/images/TRAIN/\"\n",
    "test_dataset_path = \"/images/TEST/\"\n",
    "pred_dataset_path = \"/images/TEST_SIMPLE/\"\n",
    "\n",
    "list_dir = os.listdir(train_dataset_path)\n",
    "num_classes = len(list_dir)\n",
    "batch = 16\n",
    "classes = ['EOSINOPHIL','LYMPHOCYTE','MONOCYTE','NEUTROPHIL']\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use the function to go through all the folders in the training and testing folders and separating the images into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dataset_path,\n",
    "        target_size=(240, 320),\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dataset_path,\n",
    "        target_size=(240, 320),\n",
    "        batch_size=batch,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start defining our model. In this example, we'll be using the premade VGG19 model and finetuning it to our needs. So after importing it, we first declare an instance of the model, along with what weights we want to use, and the input shape that the image should be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_conv = VGG19(weights='imagenet',\n",
    "                 include_top=False,\n",
    "                 input_shape=(240, 320, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make certain layers trainable. When using a preestablished neural network architecture, by default the layers in the architecture are \"frozen\" meaning they won't be trained. Frequently networks are used like this and a final classification layer is added to the network to fit the dataset currently being worked on. It is also common for certain layers to be \"unfrozen\", or made trainable. It is quite easy to make layers trainable, you just need to declare that the layers you want are unfrozen. \n",
    "\n",
    "We'll declare that everything but the last five layers of the model aren't trainable, which makes the last five trainable. We can confirm this by printing just the trainable layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg_conv.layers[:-14]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just making some layers in the network trainable (unfreeezing them), we can also add layers to the base model. This is done by simply declaring a new model architecture with the `sequential` function, adding in the pretrained model, and then adding new layers on top of that. We can then be sure that the new model contains both the premade layers and the new layers we've specified by printing the `summary()` of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Sequential()\n",
    "new_model.add(vgg_conv)\n",
    "new_model.add(Flatten())\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Dense(256, activation='relu'))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Dense(128, activation='relu'))\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Dense(64, activation='relu'))\n",
    "new_model.add(BatchNormalization())\n",
    "new_model.add(Dropout(0.2))\n",
    "new_model.add(Dense(len(classes), activation='softmax'))\n",
    "print(new_model.summary())\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit the model, we'll define some callbacks. We want to have a checkpoint system in place, creating checkpoints when performance on the validation set improves. We'll also be adjusting the learning rate for the classifier, reducing it whenever it hits a pleateu, and we use the `ReduceLROnPleateu` function for this. Finally, we'll enable early stopiing, allowing the model to stop training early if it stops making significant progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"weights_vgg19.hdf5\"\n",
    "callbacks = [ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max'),\n",
    "              ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, mode='min', min_lr=0.00001),\n",
    "              EarlyStopping(monitor= 'val_loss', min_delta=1e-10, patience=15, verbose=1, restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model with our chosen arguments, including our callbacks, chosen number of training epochs, and the test data as the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try \"train_generator.n // train_generator.batch_size \" for steps per epoch and validation steps\n",
    "train_records = new_model.fit_generator(train_generator,\n",
    "         epochs = 80,\n",
    "         steps_per_epoch= 100,\n",
    "         validation_data = test_generator,\n",
    "         validation_steps= 100,\n",
    "         callbacks = callbacks,\n",
    "         verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate the performance of our classifier. Let's visualize how the loss and accuarcy change over time on the validation and training sets. First, we're going to select what variables/statistics we are interested in analyzing. This will be the accuracy and the loss for both the training and validation sets. We'll also be needing the length of time the model was trained for, so we'll get the length of the training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training and validation accuracy/loss\n",
    "# declare important variables\n",
    "training_acc = train_records.history['acc']\n",
    "val_acc = train_records.history['val_acc']\n",
    "training_loss = train_records.history['loss']\n",
    "validation_loss = train_records.history['val_loss']\n",
    "\n",
    "# gets the length of how long the model was trained for\n",
    "num_epochs = range(1, len(training_acc) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we ought to be able to plot the loss and accuracy across the number of epochs we were training for. First, let's plot the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss across the number of epochs\n",
    "plt.figure()\n",
    "plt.plot(num_epochs, training_loss, label='Training Loss')\n",
    "plt.plot(num_epochs, validation_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(num_epochs, training_acc, label='Training Accuracy')\n",
    "plt.plot(num_epochs, val_acc, label='Training Accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate generator\n",
    "score = new_model.evaluate_generator(test_generator, verbose=0, steps=32)\n",
    "print('\\nAchieved Accuracy:', score[1],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go, we've implemented a fine-tuned CNN on a dataset, and then checked how the CNN performed, all with relatively few lines of code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
